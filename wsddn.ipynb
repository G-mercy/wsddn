{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e88cf1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-18T11:01:35.530720Z",
     "iopub.status.busy": "2024-06-18T11:01:35.530361Z",
     "iopub.status.idle": "2024-06-18T11:01:42.239974Z",
     "shell.execute_reply": "2024-06-18T11:01:42.239162Z"
    },
    "papermill": {
     "duration": 6.717083,
     "end_time": "2024-06-18T11:01:42.242353",
     "exception": false,
     "start_time": "2024-06-18T11:01:35.525270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from math import floor\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "class myDataSet(data.Dataset):\n",
    "    def __init__(self,\n",
    "                 ssw_path:str,\n",
    "                 transform,\n",
    "                 cat_num:int =102,\n",
    "                 R: int =20):\n",
    "        self.transform = transform\n",
    "        self.imgs = []\n",
    "        with open(ssw_path, 'r') as f:\n",
    "            for wordss in f:\n",
    "                wordss = wordss.rstrip().split()\n",
    "                label_cur = [0 for i in range(cat_num)]\n",
    "                cat_id = int(wordss[0].split('/')[-2])\n",
    "                label_cur[cat_id-1] = 1\n",
    "                # print(cat_id)\n",
    "                num_blocks = floor((len(wordss) - 1) / 4)\n",
    "                ssw_block = torch.Tensor(R, 4)\n",
    "                for i in range(R):\n",
    "                    if i<num_blocks:\n",
    "                        w = max(int(wordss[i * 4 + 3]), 2)\n",
    "                        h = max(int(wordss[i * 4 + 4]), 2)\n",
    "                        ssw_block[i, 0] = (\n",
    "                            30 - w if (int(wordss[i * 4 + 1]) + w >= 31) else int(wordss[i * 4 + 1]))\n",
    "                        ssw_block[i, 2] = w\n",
    "                        ssw_block[i, 1] = (\n",
    "                            30 - h if (int(wordss[i * 4 + 2]) + h >= 31) else int(wordss[i * 4 + 2]))\n",
    "                        ssw_block[i, 3] = h\n",
    "                    else:\n",
    "                        ssw_block[i] = torch.tensor([0,0,2,2])\n",
    "                self.imgs.append([wordss[0], ssw_block, label_cur])\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cur_img = Image.open(self.imgs[index][0])\n",
    "        data_once = self.transform(cur_img)\n",
    "        ssw_block = self.imgs[index][1]\n",
    "        label_once = self.imgs[index][2]\n",
    "        return data_once, ssw_block, torch.Tensor(label_once)\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "def random_resize(image, size, scale_range):\n",
    "    '''随机缩放图像大小的函数'''\n",
    "    # 随机选择一个缩放因子\n",
    "    scale_factor = random.uniform(*scale_range)\n",
    "    # 计算新的尺寸\n",
    "    new_size = int(size * scale_factor)\n",
    "    # 使用transforms.Resize调整图像大小\n",
    "    resize_transform = transforms.Resize((new_size, new_size))\n",
    "    return resize_transform(image)\n",
    "\n",
    "class RandomResize(object):\n",
    "    def __init__(self, size, scale_range):\n",
    "        self.size=size\n",
    "        self.scale_range=scale_range\n",
    "    def __call__(self, image):\n",
    "        '''随机缩放图像大小的函数'''\n",
    "        # 随机选择一个缩放因子\n",
    "        scale_factor = random.uniform(*self.scale_range)\n",
    "        # 计算新的尺寸\n",
    "        new_size = int(self.size * scale_factor)\n",
    "        # 使用transforms.Resize调整图像大小\n",
    "        resize_transform = transforms.Resize((new_size, new_size))\n",
    "        return resize_transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a22de8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T11:01:42.249895Z",
     "iopub.status.busy": "2024-06-18T11:01:42.249480Z",
     "iopub.status.idle": "2024-06-18T11:01:42.273657Z",
     "shell.execute_reply": "2024-06-18T11:01:42.272725Z"
    },
    "papermill": {
     "duration": 0.030327,
     "end_time": "2024-06-18T11:01:42.275721",
     "exception": false,
     "start_time": "2024-06-18T11:01:42.245394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as v_models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import floor\n",
    "import math\n",
    "\n",
    "def spatial_pyramid_pool(previous_conv, num_sample, previous_conv_size, out_pool_size):\n",
    "    '''\n",
    "    previous_conv: a tensor vector of previous convolution layer\n",
    "    num_sample: an int number of image in the batch\n",
    "    previous_conv_size: an int vector [height, width] of the matrix features size of previous convolution layer\n",
    "    out_pool_size: a int vector of expected output size of max pooling layer\n",
    "    \n",
    "    returns: a tensor vector with shape [1 x n] is the concentration of multi-level pooling\n",
    "    '''    \n",
    "    # print(previous_conv.size())\n",
    "    for i in range(len(out_pool_size)):\n",
    "        # print(previous_conv_size)\n",
    "        h_wid = math.ceil(previous_conv_size[0] / out_pool_size[i])\n",
    "        w_wid = math.ceil(previous_conv_size[1] / out_pool_size[i])\n",
    "        h_pad = min(math.floor((h_wid*out_pool_size[i] - previous_conv_size[0] + 1)/2),math.floor(h_wid/2))\n",
    "        w_pad = min(math.floor((w_wid*out_pool_size[i] - previous_conv_size[1] + 1)/2),math.floor(w_wid/2))\n",
    "        #print([h_wid,w_wid,h_pad,w_pad])\n",
    "        maxpool = nn.MaxPool2d((h_wid, w_wid), stride=(h_wid, w_wid), padding=(h_pad, w_pad))\n",
    "        x = maxpool(previous_conv)\n",
    "        if(i == 0):\n",
    "            spp = x.view(num_sample,-1)\n",
    "            # print(\"spp size:\",spp.size())\n",
    "        else:\n",
    "            # print(\"size:\",spp.size())\n",
    "            spp = torch.cat((spp,x.view(num_sample,-1)), 1)\n",
    "    return spp\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
    "}\n",
    "\n",
    "\n",
    "class WSDDN(nn.Module):\n",
    "    def __init__(self, vgg_name, cat_num):\n",
    "        super(WSDDN, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.fc6 = nn.Linear(4096, 4096)\n",
    "        self.fc7 = nn.Linear(4096, 4096)\n",
    "        self.fc8c = nn.Linear(4096, cat_num)\n",
    "        self.fc8d = nn.Linear(4096, cat_num)\n",
    "\n",
    "    def forward(self, x, ssw_bbox):  # x.shape = [BATCH_SIZE, c, h, w]  ssw_bbox.shape = [BATCH_SIZE, R, 4]\n",
    "        # print(\"input\",end=':')\n",
    "        # print(x.shape)#[1, 3, 480, 480]\n",
    "        # print(\"ssw_bbox\", end=':')\n",
    "        # print(ssw_bbox.shape)  #[1, 24, 4]\n",
    "        x = self.features(x)\n",
    "        # print(x.shape)#[1, 512, 30, 30]\n",
    "        x = self.through_spp_new(x, ssw_bbox)\n",
    "        # print(\"through_spp\",end=':')\n",
    "        # print(x.shape)#[1, 24, 4096]\n",
    "        x = nn.LeakyReLU()(self.fc6(x))\n",
    "        x = nn.LeakyReLU()(self.fc7(x))\n",
    "        x_c = nn.LeakyReLU()(self.fc8c(x))\n",
    "        x_d = nn.LeakyReLU()(self.fc8d(x))\n",
    "        # print(\"x_c\", end=':')\n",
    "        # print(x_c.shape)#[1, 24, 102]\n",
    "        # print(\"x_d\", end=':')\n",
    "        # print(x_d.shape)#[1, 24, 102]\n",
    "        segma_c = F.softmax(x_c, dim=2)\n",
    "        segma_d = F.softmax(x_d, dim=1)\n",
    "        # print(\"segma_c\", end=':')\n",
    "        # print(segma_c.shape)#[1, 24, 102]\n",
    "        # print(\"segma_d\", end=':')\n",
    "        # print(segma_d.shape)#[1, 24, 102]\n",
    "        y = torch.mul(segma_c, segma_d)\n",
    "        y = torch.sum(y, dim=1)\n",
    "        # print(\"y\", end=':')\n",
    "        # print(y.shape)#[1, 102]\n",
    "        return y, segma_c, segma_d\n",
    "\n",
    "    def _make_layers(self, cfg):  # init VGG\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def through_spp_new(self, x,\n",
    "                        ssw):  # x.shape = [BATCH_SIZE, 512, 14, 14] ssw_bbox.shape = [BATCH_SIZE, R, 4] y.shape = [BATCH_SIZE, R, 4096]\n",
    "        for i in range(x.size(0)):\n",
    "            for j in range(ssw.size(1)):\n",
    "                fmap_piece = torch.unsqueeze(x[i, :, floor(ssw[i, j, 0]): floor(ssw[i, j, 0] + ssw[i, j, 2]),\n",
    "                                             floor(ssw[i, j, 1]): floor(ssw[i, j, 1] + ssw[i, j, 3])], 0)\n",
    "                fmap_piece = spatial_pyramid_pool(previous_conv=fmap_piece, num_sample=1,\n",
    "                                                  previous_conv_size=[fmap_piece.size(2), fmap_piece.size(3)],\n",
    "                                                  out_pool_size=[2, 2])\n",
    "                if j == 0:\n",
    "                    y_piece = fmap_piece\n",
    "                    # print('fmap_piece.shape', fmap_piece.shape)\n",
    "                else:\n",
    "\n",
    "                    y_piece = torch.cat((y_piece, fmap_piece))\n",
    "            if i == 0:\n",
    "                y = torch.unsqueeze(y_piece, 0)\n",
    "                # print('y_piece', y_piece.shape)\n",
    "            else:\n",
    "                y = torch.cat((y, torch.unsqueeze(y_piece, 0)))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d3c621",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T11:01:42.282957Z",
     "iopub.status.busy": "2024-06-18T11:01:42.282652Z",
     "iopub.status.idle": "2024-06-18T11:54:19.660716Z",
     "shell.execute_reply": "2024-06-18T11:54:19.659525Z"
    },
    "papermill": {
     "duration": 3157.385006,
     "end_time": "2024-06-18T11:54:19.663486",
     "exception": false,
     "start_time": "2024-06-18T11:01:42.278480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: s3re0wrs\n",
      "Sweep URL: https://wandb.ai/1844986810/WSDDN/sweeps/s3re0wrs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z0s1x4ya with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcat_num: 102\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearn_rate: 6.4e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_path: /kaggle/working/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_workers: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpreCNN_name: VGG11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tresult_name: wsddn.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_data_path: /kaggle/input/wsddn-od/ssw_train.txt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tval_data_path: /kaggle/input/wsddn-od/ssw_val.txt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 1e-07\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m1844986810\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240618_110146-z0s1x4ya\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meffortless-sweep-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/1844986810/WSDDN\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/1844986810/WSDDN/sweeps/s3re0wrs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/1844986810/WSDDN/runs/z0s1x4ya\u001b[0m\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training at 11:02:09\n",
      "Epoch 1\\15\n",
      "[11:02:18] [10/  328] ETA: 00:01:08:57 loss: 9.773\n",
      "[11:02:23] [20/  328] ETA: 00:00:58:01 loss: 5.915\n",
      "[11:02:29] [30/  328] ETA: 00:00:54:10 loss: 5.856\n",
      "[11:02:35] [40/  328] ETA: 00:00:52:16 loss: 4.604\n",
      "[11:02:41] [50/  328] ETA: 00:00:51:11 loss: 3.383\n",
      "[11:02:46] [60/  328] ETA: 00:00:50:19 loss: 3.639\n",
      "[11:02:52] [70/  328] ETA: 00:00:49:43 loss: 2.502\n",
      "[11:02:58] [80/  328] ETA: 00:00:49:09 loss: 2.250\n",
      "[11:03:04] [90/  328] ETA: 00:00:48:43 loss: 3.091\n",
      "[11:03:10] [100/  328] ETA: 00:00:48:26 loss: 3.890\n",
      "[11:03:15] [110/  328] ETA: 00:00:48:10 loss: 3.775\n",
      "[11:03:21] [120/  328] ETA: 00:00:47:53 loss: 1.567\n",
      "[11:03:27] [130/  328] ETA: 00:00:47:39 loss: 3.053\n",
      "[11:03:33] [140/  328] ETA: 00:00:47:25 loss: 2.893\n",
      "[11:03:38] [150/  328] ETA: 00:00:47:12 loss: 1.361\n",
      "[11:03:44] [160/  328] ETA: 00:00:47:02 loss: 1.884\n",
      "[11:03:50] [170/  328] ETA: 00:00:46:51 loss: 2.334\n",
      "[11:03:56] [180/  328] ETA: 00:00:46:40 loss: 2.903\n",
      "[11:04:01] [190/  328] ETA: 00:00:46:30 loss: 1.464\n",
      "[11:04:07] [200/  328] ETA: 00:00:46:19 loss: 1.632\n",
      "[11:04:13] [210/  328] ETA: 00:00:46:12 loss: 1.593\n",
      "[11:04:19] [220/  328] ETA: 00:00:46:04 loss: 2.480\n",
      "[11:04:24] [230/  328] ETA: 00:00:45:55 loss: 1.399\n",
      "[11:04:30] [240/  328] ETA: 00:00:45:49 loss: 1.523\n",
      "[11:04:36] [250/  328] ETA: 00:00:45:42 loss: 1.694\n",
      "[11:04:42] [260/  328] ETA: 00:00:45:34 loss: 2.238\n",
      "[11:04:48] [270/  328] ETA: 00:00:45:27 loss: 2.111\n",
      "[11:04:53] [280/  328] ETA: 00:00:45:20 loss: 1.439\n",
      "[11:04:59] [290/  328] ETA: 00:00:45:13 loss: 1.180\n",
      "[11:05:05] [300/  328] ETA: 00:00:45:05 loss: 1.821\n",
      "[11:05:11] [310/  328] ETA: 00:00:45:00 loss: 0.655\n",
      "[11:05:16] [320/  328] ETA: 00:00:44:51 loss: 1.623\n",
      "[11:05:21] [328/  328] ETA: 00:00:44:41 loss: 1.929\n",
      "Start Testing at 11:05:21\n",
      "[11:05:24] [10/   82] ETA: 00:00:07:21 loss: 1.790\n",
      "[11:05:27] [20/   82] ETA: 00:00:06:10 loss: 1.406\n",
      "[11:05:29] [30/   82] ETA: 00:00:05:41 loss: 0.774\n",
      "[11:05:32] [40/   82] ETA: 00:00:05:32 loss: 0.591\n",
      "[11:05:35] [50/   82] ETA: 00:00:05:24 loss: 1.863\n",
      "[11:05:37] [60/   82] ETA: 00:00:05:15 loss: 1.370\n",
      "[11:05:39] [70/   82] ETA: 00:00:05:08 loss: 1.194\n",
      "[11:05:42] [80/   82] ETA: 00:00:05:01 loss: 2.369\n",
      "[11:05:42] [82/   82] ETA: 00:00:05:00 loss: 0.631\n",
      "Classification Accuracy of the model on the test images(mAcc): 0.9886 %\n",
      "Finished Testing\n",
      "Finish Epoch 1\\15 with acc: 0.986\n",
      "Epoch 2\\15\n",
      "[11:05:50] [10/  328] ETA: 01:06:04:07 loss: 0.310\n",
      "[11:05:55] [20/  328] ETA: 00:15:23:10 loss: 0.313\n",
      "[11:06:01] [30/  328] ETA: 00:10:29:30 loss: 0.793\n",
      "[11:06:07] [40/  328] ETA: 00:08:02:31 loss: 0.372\n",
      "[11:06:12] [50/  328] ETA: 00:06:34:27 loss: 0.571\n",
      "[11:06:18] [60/  328] ETA: 00:05:35:40 loss: 0.458\n",
      "[11:06:24] [70/  328] ETA: 00:04:53:44 loss: 0.225\n",
      "[11:06:29] [80/  328] ETA: 00:04:22:06 loss: 0.324\n",
      "[11:06:35] [90/  328] ETA: 00:03:57:32 loss: 0.661\n",
      "[11:06:40] [100/  328] ETA: 00:03:37:51 loss: 0.388\n",
      "[11:06:46] [110/  328] ETA: 00:03:21:43 loss: 1.292\n",
      "[11:06:52] [120/  328] ETA: 00:03:08:22 loss: 0.872\n",
      "[11:06:57] [130/  328] ETA: 00:02:56:56 loss: 0.355\n",
      "[11:07:03] [140/  328] ETA: 00:02:47:08 loss: 0.587\n",
      "[11:07:09] [150/  328] ETA: 00:02:38:38 loss: 1.520\n",
      "[11:07:14] [160/  328] ETA: 00:02:31:11 loss: 3.836\n",
      "[11:07:20] [170/  328] ETA: 00:02:24:38 loss: 0.462\n",
      "[11:07:25] [180/  328] ETA: 00:02:18:47 loss: 2.830\n",
      "[11:07:31] [190/  328] ETA: 00:02:13:33 loss: 0.809\n",
      "[11:07:37] [200/  328] ETA: 00:02:08:48 loss: 1.194\n",
      "[11:07:42] [210/  328] ETA: 00:02:04:30 loss: 0.720\n",
      "[11:07:48] [220/  328] ETA: 00:02:00:36 loss: 0.463\n",
      "[11:07:54] [230/  328] ETA: 00:01:57:01 loss: 0.918\n",
      "[11:07:59] [240/  328] ETA: 00:01:53:45 loss: 0.387\n",
      "[11:08:05] [250/  328] ETA: 00:01:50:42 loss: 0.328\n",
      "[11:08:10] [260/  328] ETA: 00:01:47:53 loss: 0.850\n",
      "[11:08:16] [270/  328] ETA: 00:01:45:17 loss: 0.152\n",
      "[11:08:22] [280/  328] ETA: 00:01:42:51 loss: 0.151\n",
      "[11:08:27] [290/  328] ETA: 00:01:40:35 loss: 0.314\n",
      "[11:08:33] [300/  328] ETA: 00:01:38:28 loss: 0.956\n",
      "[11:08:38] [310/  328] ETA: 00:01:36:28 loss: 0.567\n",
      "[11:08:44] [320/  328] ETA: 00:01:34:35 loss: 0.203\n",
      "[11:08:48] [328/  328] ETA: 00:01:33:06 loss: 0.722\n",
      "Start Testing at 11:08:48\n",
      "[11:08:52] [10/   82] ETA: 00:00:07:58 loss: 0.473\n",
      "[11:08:55] [20/   82] ETA: 00:00:06:31 loss: 1.294\n",
      "[11:08:57] [30/   82] ETA: 00:00:05:57 loss: 0.272\n",
      "[11:09:00] [40/   82] ETA: 00:00:05:37 loss: 0.315\n",
      "[11:09:02] [50/   82] ETA: 00:00:05:24 loss: 0.835\n",
      "[11:09:04] [60/   82] ETA: 00:00:05:15 loss: 1.281\n",
      "[11:09:07] [70/   82] ETA: 00:00:05:07 loss: 0.999\n",
      "[11:09:09] [80/   82] ETA: 00:00:05:00 loss: 1.940\n",
      "[11:09:10] [82/   82] ETA: 00:00:04:59 loss: 0.205\n",
      "Classification Accuracy of the model on the test images(mAcc): 0.9909 %\n",
      "Finished Testing\n",
      "Finish Epoch 2\\15 with acc: 0.992\n",
      "Epoch 3\\15\n",
      "[11:09:17] [10/  328] ETA: 02:10:22:08 loss: 0.130\n",
      "[11:09:23] [20/  328] ETA: 01:05:30:58 loss: 0.296\n",
      "[11:09:29] [30/  328] ETA: 00:19:53:54 loss: 0.077\n",
      "[11:09:34] [40/  328] ETA: 00:15:05:07 loss: 0.153\n",
      "[11:09:40] [50/  328] ETA: 00:12:11:42 loss: 0.191\n",
      "[11:09:46] [60/  328] ETA: 00:10:16:04 loss: 0.166\n",
      "[11:09:51] [70/  328] ETA: 00:08:53:26 loss: 0.334\n",
      "[11:09:57] [80/  328] ETA: 00:07:51:27 loss: 0.090\n",
      "[11:10:02] [90/  328] ETA: 00:07:03:15 loss: 0.403\n",
      "[11:10:08] [100/  328] ETA: 00:06:24:39 loss: 0.173\n",
      "[11:10:14] [110/  328] ETA: 00:05:53:01 loss: 0.306\n",
      "[11:10:19] [120/  328] ETA: 00:05:26:38 loss: 0.133\n",
      "[11:10:25] [130/  328] ETA: 00:05:04:20 loss: 0.030\n",
      "[11:10:31] [140/  328] ETA: 00:04:45:15 loss: 0.350\n",
      "[11:10:36] [150/  328] ETA: 00:04:28:38 loss: 0.090\n",
      "[11:10:42] [160/  328] ETA: 00:04:14:06 loss: 0.145\n",
      "[11:10:47] [170/  328] ETA: 00:04:01:15 loss: 0.127\n",
      "[11:10:53] [180/  328] ETA: 00:03:49:49 loss: 0.073\n",
      "[11:10:58] [190/  328] ETA: 00:03:39:35 loss: 0.158\n",
      "[11:11:04] [200/  328] ETA: 00:03:30:25 loss: 0.047\n",
      "[11:11:10] [210/  328] ETA: 00:03:22:04 loss: 0.073\n",
      "[11:11:15] [220/  328] ETA: 00:03:14:28 loss: 0.045\n",
      "[11:11:21] [230/  328] ETA: 00:03:07:32 loss: 0.065\n",
      "[11:11:27] [240/  328] ETA: 00:03:01:09 loss: 0.248\n",
      "[11:11:32] [250/  328] ETA: 00:02:55:20 loss: 0.203\n",
      "[11:11:38] [260/  328] ETA: 00:02:49:55 loss: 0.422\n",
      "[11:11:44] [270/  328] ETA: 00:02:44:52 loss: 1.600\n",
      "[11:11:49] [280/  328] ETA: 00:02:40:11 loss: 1.851\n",
      "[11:11:55] [290/  328] ETA: 00:02:35:49 loss: 0.439\n",
      "[11:12:01] [300/  328] ETA: 00:02:31:45 loss: 1.035\n",
      "[11:12:06] [310/  328] ETA: 00:02:27:58 loss: 0.095\n",
      "[11:12:12] [320/  328] ETA: 00:02:24:23 loss: 0.176\n",
      "[11:12:16] [328/  328] ETA: 00:02:21:36 loss: 0.183\n",
      "Start Testing at 11:12:16\n",
      "[11:12:20] [10/   82] ETA: 00:00:08:06 loss: 1.106\n",
      "[11:12:23] [20/   82] ETA: 00:00:06:41 loss: 1.100\n",
      "[11:12:25] [30/   82] ETA: 00:00:06:06 loss: 0.318\n",
      "[11:12:28] [40/   82] ETA: 00:00:05:47 loss: 0.681\n",
      "[11:12:30] [50/   82] ETA: 00:00:05:34 loss: 0.342\n",
      "[11:12:33] [60/   82] ETA: 00:00:05:22 loss: 0.910\n",
      "[11:12:35] [70/   82] ETA: 00:00:05:17 loss: 1.145\n",
      "[11:12:38] [80/   82] ETA: 00:00:05:08 loss: 2.131\n",
      "[11:12:38] [82/   82] ETA: 00:00:05:07 loss: 0.170\n",
      "Classification Accuracy of the model on the test images(mAcc): 0.9893 %\n",
      "Finished Testing\n",
      "Finish Epoch 3\\15 with acc: 0.997\n",
      "Epoch 4\\15\n",
      "[11:12:45] [10/  328] ETA: 03:14:45:08 loss: 0.152\n",
      "[11:12:51] [20/  328] ETA: 01:19:39:59 loss: 0.119\n",
      "[11:12:56] [30/  328] ETA: 01:05:18:17 loss: 0.063\n",
      "[11:13:02] [40/  328] ETA: 00:22:07:34 loss: 0.219\n",
      "[11:13:08] [50/  328] ETA: 00:17:49:01 loss: 0.549\n",
      "[11:13:13] [60/  328] ETA: 00:14:56:39 loss: 0.105\n",
      "[11:13:19] [70/  328] ETA: 00:12:53:26 loss: 0.291\n",
      "[11:13:25] [80/  328] ETA: 00:11:21:00 loss: 0.060\n",
      "[11:13:30] [90/  328] ETA: 00:10:09:07 loss: 0.027\n",
      "[11:13:36] [100/  328] ETA: 00:09:11:36 loss: 0.755\n",
      "[11:13:42] [110/  328] ETA: 00:08:24:37 loss: 0.084\n",
      "[11:13:47] [120/  328] ETA: 00:07:45:22 loss: 0.063\n",
      "[11:13:53] [130/  328] ETA: 00:07:12:08 loss: 0.078\n",
      "[11:13:58] [140/  328] ETA: 00:06:43:37 loss: 0.100\n",
      "[11:14:04] [150/  328] ETA: 00:06:18:55 loss: 0.042\n",
      "[11:14:10] [160/  328] ETA: 00:05:57:18 loss: 0.030\n",
      "[11:14:15] [170/  328] ETA: 00:05:38:11 loss: 0.019\n",
      "[11:14:21] [180/  328] ETA: 00:05:21:11 loss: 0.101\n",
      "[11:14:27] [190/  328] ETA: 00:05:05:58 loss: 0.042\n",
      "[11:14:32] [200/  328] ETA: 00:04:52:15 loss: 0.136\n",
      "[11:14:38] [210/  328] ETA: 00:04:39:50 loss: 0.255\n",
      "[11:14:44] [220/  328] ETA: 00:04:28:35 loss: 0.041\n",
      "[11:14:49] [230/  328] ETA: 00:04:18:15 loss: 0.084\n",
      "[11:14:55] [240/  328] ETA: 00:04:08:48 loss: 0.115\n",
      "[11:15:00] [250/  328] ETA: 00:04:00:05 loss: 0.195\n",
      "[11:15:06] [260/  328] ETA: 00:03:52:03 loss: 0.060\n",
      "[11:15:12] [270/  328] ETA: 00:03:44:36 loss: 0.035\n",
      "[11:15:17] [280/  328] ETA: 00:03:37:40 loss: 0.193\n",
      "[11:15:23] [290/  328] ETA: 00:03:31:13 loss: 0.039\n",
      "[11:15:29] [300/  328] ETA: 00:03:25:10 loss: 0.008\n",
      "[11:15:34] [310/  328] ETA: 00:03:19:31 loss: 0.028\n",
      "[11:15:40] [320/  328] ETA: 00:03:14:12 loss: 0.142\n",
      "[11:15:44] [328/  328] ETA: 00:03:10:07 loss: 0.554\n",
      "Start Testing at 11:15:44\n",
      "[11:15:48] [10/   82] ETA: 00:00:07:32 loss: 0.938\n",
      "[11:15:50] [20/   82] ETA: 00:00:06:12 loss: 0.633\n",
      "[11:15:53] [30/   82] ETA: 00:00:05:43 loss: 0.193\n",
      "[11:15:55] [40/   82] ETA: 00:00:05:26 loss: 0.268\n",
      "[11:15:58] [50/   82] ETA: 00:00:05:16 loss: 0.237\n",
      "[11:16:00] [60/   82] ETA: 00:00:05:07 loss: 0.297\n",
      "[11:16:02] [70/   82] ETA: 00:00:05:00 loss: 1.174\n",
      "[11:16:05] [80/   82] ETA: 00:00:04:54 loss: 2.172\n",
      "[11:16:05] [82/   82] ETA: 00:00:04:53 loss: 0.141\n",
      "Classification Accuracy of the model on the test images(mAcc): 0.9931 %\n",
      "Finished Testing\n",
      "Finish Epoch 4\\15 with acc: 0.998\n",
      "Epoch 5\\15\n",
      "[11:16:13] [10/  328] ETA: 04:19:02:20 loss: 0.090\n",
      "[11:16:18] [20/  328] ETA: 02:09:47:23 loss: 0.187\n",
      "[11:16:24] [30/  328] ETA: 01:14:42:08 loss: 0.363\n",
      "[11:16:30] [40/  328] ETA: 01:05:09:29 loss: 0.060\n",
      "[11:16:35] [50/  328] ETA: 00:23:25:49 loss: 0.206\n",
      "[11:16:41] [60/  328] ETA: 00:19:36:38 loss: 0.218\n",
      "[11:16:46] [70/  328] ETA: 00:16:53:00 loss: 0.095\n",
      "[11:16:52] [80/  328] ETA: 00:14:50:17 loss: 0.444\n",
      "[11:16:58] [90/  328] ETA: 00:13:14:43 loss: 0.027\n",
      "[11:17:03] [100/  328] ETA: 00:11:58:17 loss: 0.957\n",
      "[11:17:09] [110/  328] ETA: 00:10:55:42 loss: 0.255\n",
      "[11:17:15] [120/  328] ETA: 00:10:03:35 loss: 0.080\n",
      "[11:17:20] [130/  328] ETA: 00:09:19:29 loss: 0.086\n",
      "[11:17:26] [140/  328] ETA: 00:08:41:41 loss: 1.770\n",
      "[11:17:32] [150/  328] ETA: 00:08:08:53 loss: 0.035\n",
      "[11:17:37] [160/  328] ETA: 00:07:40:08 loss: 0.052\n",
      "[11:17:43] [170/  328] ETA: 00:07:14:46 loss: 0.175\n",
      "[11:17:49] [180/  328] ETA: 00:06:52:15 loss: 0.055\n",
      "[11:17:54] [190/  328] ETA: 00:06:32:04 loss: 0.061\n",
      "[11:18:00] [200/  328] ETA: 00:06:13:53 loss: 0.377\n",
      "[11:18:05] [210/  328] ETA: 00:05:57:24 loss: 0.157\n",
      "[11:18:11] [220/  328] ETA: 00:05:42:26 loss: 0.051\n",
      "[11:18:17] [230/  328] ETA: 00:05:28:45 loss: 0.131\n",
      "[11:18:22] [240/  328] ETA: 00:05:16:13 loss: 0.060\n",
      "[11:18:28] [250/  328] ETA: 00:05:04:40 loss: 0.076\n",
      "[11:18:33] [260/  328] ETA: 00:04:54:00 loss: 0.037\n",
      "[11:18:39] [270/  328] ETA: 00:04:44:06 loss: 0.055\n",
      "[11:18:45] [280/  328] ETA: 00:04:34:55 loss: 0.170\n",
      "[11:18:50] [290/  328] ETA: 00:04:26:22 loss: 0.017\n",
      "[11:18:56] [300/  328] ETA: 00:04:18:22 loss: 0.029\n",
      "[11:19:02] [310/  328] ETA: 00:04:10:53 loss: 0.023\n",
      "[11:19:07] [320/  328] ETA: 00:04:03:51 loss: 0.079\n",
      "[11:19:11] [328/  328] ETA: 00:03:58:29 loss: 0.065\n",
      "Start Testing at 11:19:11\n",
      "[11:19:15] [10/   82] ETA: 00:00:07:38 loss: 0.596\n",
      "[11:19:18] [20/   82] ETA: 00:00:06:13 loss: 2.029\n",
      "[11:19:20] [30/   82] ETA: 00:00:05:54 loss: 0.208\n",
      "[11:19:23] [40/   82] ETA: 00:00:05:45 loss: 0.508\n",
      "[11:19:26] [50/   82] ETA: 00:00:05:34 loss: 0.198\n",
      "[11:19:28] [60/   82] ETA: 00:00:05:24 loss: 0.850\n",
      "[11:19:30] [70/   82] ETA: 00:00:05:16 loss: 0.471\n",
      "[11:19:33] [80/   82] ETA: 00:00:05:07 loss: 1.959\n",
      "[11:19:33] [82/   82] ETA: 00:00:05:06 loss: 0.376\n",
      "Classification Accuracy of the model on the test images(mAcc): 0.9924 %\n",
      "Finished Testing\n",
      "Finish Epoch 5\\15 with acc: 0.998\n",
      "Epoch 6\\15\n",
      "[11:19:40] [10/  328] ETA: 05:23:21:21 loss: 0.014\n",
      "[11:19:46] [20/  328] ETA: 02:23:54:49 loss: 0.034\n",
      "[11:19:52] [30/  328] ETA: 02:00:06:04 loss: 0.013\n",
      "[11:19:57] [40/  328] ETA: 01:12:11:39 loss: 0.012\n",
      "[11:20:03] [50/  328] ETA: 01:05:02:58 loss: 0.037\n",
      "[11:20:09] [60/  328] ETA: 01:00:17:05 loss: 0.014\n",
      "[11:20:14] [70/  328] ETA: 00:20:52:52 loss: 0.022\n",
      "[11:20:20] [80/  328] ETA: 00:18:19:42 loss: 0.016\n",
      "[11:20:26] [90/  328] ETA: 00:16:20:37 loss: 0.027\n",
      "[11:20:31] [100/  328] ETA: 00:14:45:15 loss: 0.021\n",
      "[11:20:37] [110/  328] ETA: 00:13:27:13 loss: 0.013\n",
      "[11:20:43] [120/  328] ETA: 00:12:22:12 loss: 0.016\n",
      "[11:20:48] [130/  328] ETA: 00:11:27:06 loss: 0.039\n",
      "[11:20:54] [140/  328] ETA: 00:10:39:54 loss: 0.036\n",
      "[11:20:59] [150/  328] ETA: 00:09:58:57 loss: 0.033\n",
      "[11:21:05] [160/  328] ETA: 00:09:23:08 loss: 0.013\n",
      "[11:21:11] [170/  328] ETA: 00:08:51:30 loss: 0.016\n",
      "[11:21:16] [180/  328] ETA: 00:08:23:23 loss: 0.023\n",
      "[11:21:22] [190/  328] ETA: 00:07:58:11 loss: 0.026\n",
      "[11:21:27] [200/  328] ETA: 00:07:35:32 loss: 0.020\n",
      "[11:21:33] [210/  328] ETA: 00:07:15:01 loss: 0.009\n",
      "[11:21:39] [220/  328] ETA: 00:06:56:22 loss: 0.017\n",
      "[11:21:44] [230/  328] ETA: 00:06:39:19 loss: 0.029\n",
      "[11:21:50] [240/  328] ETA: 00:06:23:41 loss: 0.008\n",
      "[11:21:55] [250/  328] ETA: 00:06:09:18 loss: 0.008\n",
      "[11:22:01] [260/  328] ETA: 00:05:56:02 loss: 0.017\n",
      "[11:22:07] [270/  328] ETA: 00:05:43:43 loss: 0.012\n",
      "[11:22:12] [280/  328] ETA: 00:05:32:16 loss: 0.007\n",
      "[11:22:18] [290/  328] ETA: 00:05:21:36 loss: 0.016\n",
      "[11:22:23] [300/  328] ETA: 00:05:11:39 loss: 0.024\n",
      "[11:22:29] [310/  328] ETA: 00:05:02:21 loss: 0.017\n",
      "[11:22:35] [320/  328] ETA: 00:04:53:37 loss: 0.021\n",
      "[11:22:39] [328/  328] ETA: 00:04:46:56 loss: 0.025\n",
      "Start Testing at 11:22:39\n",
      "[11:22:43] [10/   82] ETA: 00:00:07:59 loss: 0.455\n",
      "[11:22:45] [20/   82] ETA: 00:00:06:21 loss: 1.383\n",
      "[11:22:48] [30/   82] ETA: 00:00:05:47 loss: 0.102\n",
      "[11:22:50] [40/   82] ETA: 00:00:05:31 loss: 0.295\n",
      "[11:22:53] [50/   82] ETA: 00:00:05:21 loss: 0.054\n",
      "[11:22:55] [60/   82] ETA: 00:00:05:12 loss: 0.317\n",
      "[11:22:58] [70/   82] ETA: 00:00:05:05 loss: 0.550\n",
      "[11:23:00] [80/   82] ETA: 00:00:04:59 loss: 1.843\n",
      "[11:23:00] [82/   82] ETA: 00:00:04:57 loss: 0.018\n",
      "Classification Accuracy of the model on the test images(mAcc): 0.9931 %\n",
      "Finished Testing\n",
      "Finish Epoch 6\\15 with acc: 0.999\n",
      "Epoch 7\\15\n",
      "[11:23:08] [10/  328] ETA: 07:03:36:58 loss: 0.007\n",
      "[11:23:13] [20/  328] ETA: 03:14:00:52 loss: 0.011\n",
      "[11:23:19] [30/  328] ETA: 02:09:28:44 loss: 0.008\n",
      "[11:23:24] [40/  328] ETA: 01:19:12:39 loss: 0.008\n",
      "[11:23:30] [50/  328] ETA: 01:10:39:04 loss: 0.012\n",
      "[11:23:36] [60/  328] ETA: 01:04:56:32 loss: 0.005\n",
      "[11:23:41] [70/  328] ETA: 01:00:51:53 loss: 0.018\n",
      "[11:23:47] [80/  328] ETA: 00:21:48:21 loss: 0.013\n",
      "[11:23:52] [90/  328] ETA: 00:19:25:42 loss: 0.008\n",
      "[11:23:58] [100/  328] ETA: 00:17:31:27 loss: 0.068\n",
      "[11:24:04] [110/  328] ETA: 00:15:58:02 loss: 0.005\n",
      "[11:24:09] [120/  328] ETA: 00:14:40:05 loss: 0.006\n",
      "[11:24:15] [130/  328] ETA: 00:13:34:09 loss: 0.008\n",
      "[11:24:21] [140/  328] ETA: 00:12:37:36 loss: 0.022\n",
      "[11:24:26] [150/  328] ETA: 00:11:48:35 loss: 0.031\n",
      "[11:24:32] [160/  328] ETA: 00:11:05:42 loss: 0.019\n",
      "[11:24:37] [170/  328] ETA: 00:10:27:51 loss: 0.013\n",
      "[11:24:43] [180/  328] ETA: 00:09:54:11 loss: 0.015\n",
      "[11:24:49] [190/  328] ETA: 00:09:24:04 loss: 0.011\n",
      "[11:24:54] [200/  328] ETA: 00:08:56:55 loss: 0.014\n",
      "[11:25:00] [210/  328] ETA: 00:08:32:23 loss: 0.009\n",
      "[11:25:06] [220/  328] ETA: 00:08:10:05 loss: 0.013\n",
      "[11:25:11] [230/  328] ETA: 00:07:49:41 loss: 0.017\n",
      "[11:25:17] [240/  328] ETA: 00:07:30:58 loss: 0.006\n",
      "[11:25:22] [250/  328] ETA: 00:07:13:45 loss: 0.004\n",
      "[11:25:28] [260/  328] ETA: 00:06:57:52 loss: 0.008\n",
      "[11:25:34] [270/  328] ETA: 00:06:43:08 loss: 0.024\n",
      "[11:25:39] [280/  328] ETA: 00:06:29:29 loss: 0.013\n",
      "[11:25:45] [290/  328] ETA: 00:06:16:44 loss: 0.004\n",
      "[11:25:51] [300/  328] ETA: 00:06:04:50 loss: 0.009\n",
      "[11:25:56] [310/  328] ETA: 00:05:53:41 loss: 0.009\n",
      "[11:26:02] [320/  328] ETA: 00:05:43:13 loss: 0.004\n",
      "[11:26:06] [328/  328] ETA: 00:05:35:14 loss: 0.008\n",
      "Start Testing at 11:26:06\n",
      "[11:26:10] [10/   82] ETA: 00:00:08:08 loss: 0.664\n",
      "[11:26:13] [20/   82] ETA: 00:00:06:32 loss: 1.378\n",
      "[11:26:15] [30/   82] ETA: 00:00:05:55 loss: 0.136\n",
      "[11:26:17] [40/   82] ETA: 00:00:05:36 loss: 0.243\n",
      "[11:26:20] [50/   82] ETA: 00:00:05:24 loss: 0.041\n",
      "[11:26:22] [60/   82] ETA: 00:00:05:14 loss: 0.504\n",
      "[11:26:25] [70/   82] ETA: 00:00:05:06 loss: 0.672\n",
      "[11:26:27] [80/   82] ETA: 00:00:04:59 loss: 2.121\n",
      "[11:26:27] [82/   82] ETA: 00:00:04:57 loss: 0.104\n",
      "Classification Accuracy of the model on the test images(mAcc): 0.9924 %\n",
      "Finished Testing\n",
      "Finish Epoch 7\\15 with acc: 0.999\n",
      "Epoch 8\\15\n",
      "[11:26:34] [10/  328] ETA: 08:07:50:01 loss: 0.004\n",
      "[11:26:40] [20/  328] ETA: 04:04:06:05 loss: 0.009\n",
      "[11:26:46] [30/  328] ETA: 02:18:51:11 loss: 0.006\n",
      "[11:26:51] [40/  328] ETA: 02:02:13:33 loss: 0.004\n",
      "[11:26:57] [50/  328] ETA: 01:16:15:00 loss: 0.007\n",
      "[11:27:02] [60/  328] ETA: 01:09:35:55 loss: 0.007\n",
      "[11:27:08] [70/  328] ETA: 01:04:50:49 loss: 0.007\n",
      "[11:27:14] [80/  328] ETA: 01:01:17:03 loss: 0.005\n",
      "[11:27:19] [90/  328] ETA: 00:22:30:42 loss: 0.009\n",
      "[11:27:25] [100/  328] ETA: 00:20:17:37 loss: 0.003\n",
      "[11:27:31] [110/  328] ETA: 00:18:28:43 loss: 0.004\n",
      "[11:27:36] [120/  328] ETA: 00:16:57:56 loss: 0.005\n",
      "[11:27:42] [130/  328] ETA: 00:15:41:12 loss: 0.017\n",
      "[11:27:48] [140/  328] ETA: 00:14:35:22 loss: 0.004\n",
      "[11:27:53] [150/  328] ETA: 00:13:38:16 loss: 0.007\n",
      "[11:27:59] [160/  328] ETA: 00:12:48:18 loss: 0.004\n",
      "[11:28:04] [170/  328] ETA: 00:12:04:13 loss: 0.007\n",
      "[11:28:10] [180/  328] ETA: 00:11:25:00 loss: 0.005\n",
      "[11:28:16] [190/  328] ETA: 00:10:49:57 loss: 0.009\n",
      "[11:28:21] [200/  328] ETA: 00:10:18:24 loss: 0.005\n",
      "[11:28:27] [210/  328] ETA: 00:09:49:47 loss: 0.079\n",
      "[11:28:33] [220/  328] ETA: 00:09:23:47 loss: 0.008\n",
      "[11:28:38] [230/  328] ETA: 00:09:00:01 loss: 0.137\n",
      "[11:28:44] [240/  328] ETA: 00:08:38:17 loss: 0.115\n",
      "[11:28:50] [250/  328] ETA: 00:08:18:13 loss: 0.037\n",
      "[11:28:55] [260/  328] ETA: 00:07:59:42 loss: 0.168\n",
      "[11:29:01] [270/  328] ETA: 00:07:42:33 loss: 0.475\n",
      "[11:29:06] [280/  328] ETA: 00:07:26:37 loss: 0.239\n",
      "[11:29:12] [290/  328] ETA: 00:07:11:46 loss: 3.743\n",
      "[11:29:18] [300/  328] ETA: 00:06:57:56 loss: 1.477\n",
      "[11:29:23] [310/  328] ETA: 00:06:44:59 loss: 1.774\n",
      "[11:29:29] [320/  328] ETA: 00:06:32:48 loss: 0.142\n",
      "[11:29:33] [328/  328] ETA: 00:06:23:32 loss: 1.186\n",
      "Start Testing at 11:29:33\n",
      "[11:29:37] [10/   82] ETA: 00:00:08:19 loss: 0.701\n",
      "[11:29:40] [20/   82] ETA: 00:00:06:32 loss: 1.653\n",
      "[11:29:42] [30/   82] ETA: 00:00:05:57 loss: 1.708\n",
      "[11:29:44] [40/   82] ETA: 00:00:05:38 loss: 1.899\n",
      "[11:29:47] [50/   82] ETA: 00:00:05:28 loss: 0.146\n",
      "[11:29:49] [60/   82] ETA: 00:00:05:19 loss: 0.886\n",
      "[11:29:52] [70/   82] ETA: 00:00:05:11 loss: 2.068\n",
      "[11:29:54] [80/   82] ETA: 00:00:05:03 loss: 2.529\n",
      "[11:29:55] [82/   82] ETA: 00:00:05:02 loss: 0.099\n",
      "Classification Accuracy of the model on the test images(mAcc): 0.9855 %\n",
      "Finished Testing\n",
      "Finish Epoch 8\\15 with acc: 0.996\n",
      "Epoch 9\\15\n",
      "[11:30:02] [10/  328] ETA: 09:12:05:58 loss: 0.296\n",
      "[11:30:07] [20/  328] ETA: 04:18:12:00 loss: 0.320\n",
      "[11:30:13] [30/  328] ETA: 03:04:13:51 loss: 0.293\n",
      "[11:30:19] [40/  328] ETA: 02:09:15:00 loss: 0.143\n",
      "[11:30:24] [50/  328] ETA: 01:21:51:28 loss: 0.117\n",
      "[11:30:30] [60/  328] ETA: 01:14:15:44 loss: 0.286\n",
      "[11:30:35] [70/  328] ETA: 01:08:50:10 loss: 0.179\n",
      "[11:30:41] [80/  328] ETA: 01:04:45:58 loss: 0.451\n",
      "[11:30:47] [90/  328] ETA: 01:01:36:02 loss: 0.390\n",
      "[11:30:52] [100/  328] ETA: 00:23:04:10 loss: 0.073\n",
      "[11:30:58] [110/  328] ETA: 00:20:59:47 loss: 0.049\n",
      "[11:31:03] [120/  328] ETA: 00:19:16:09 loss: 0.033\n",
      "[11:31:09] [130/  328] ETA: 00:17:48:25 loss: 0.166\n",
      "[11:31:15] [140/  328] ETA: 00:16:33:12 loss: 0.102\n",
      "[11:31:20] [150/  328] ETA: 00:15:28:02 loss: 0.032\n",
      "[11:31:26] [160/  328] ETA: 00:14:30:58 loss: 0.108\n",
      "[11:31:31] [170/  328] ETA: 00:13:40:39 loss: 0.023\n",
      "[11:31:37] [180/  328] ETA: 00:12:55:53 loss: 0.081\n",
      "[11:31:43] [190/  328] ETA: 00:12:15:50 loss: 0.068\n",
      "[11:31:48] [200/  328] ETA: 00:11:39:46 loss: 0.024\n",
      "[11:31:54] [210/  328] ETA: 00:11:07:08 loss: 0.140\n",
      "[11:32:00] [220/  328] ETA: 00:10:37:27 loss: 0.013\n",
      "[11:32:05] [230/  328] ETA: 00:10:10:21 loss: 0.038\n",
      "[11:32:11] [240/  328] ETA: 00:09:45:30 loss: 0.049\n",
      "[11:32:16] [250/  328] ETA: 00:09:22:37 loss: 0.059\n",
      "[11:32:22] [260/  328] ETA: 00:09:01:31 loss: 0.026\n",
      "[11:32:28] [270/  328] ETA: 00:08:41:57 loss: 0.011\n",
      "[11:32:33] [280/  328] ETA: 00:08:23:47 loss: 0.024\n",
      "[11:32:39] [290/  328] ETA: 00:08:06:52 loss: 0.014\n",
      "[11:32:45] [300/  328] ETA: 00:07:51:04 loss: 0.033\n",
      "[11:32:50] [310/  328] ETA: 00:07:36:17 loss: 0.010\n",
      "[11:32:56] [320/  328] ETA: 00:07:22:25 loss: 0.012\n",
      "[11:33:00] [328/  328] ETA: 00:07:11:51 loss: 0.025\n",
      "Start Testing at 11:33:00\n",
      "[11:33:04] [10/   82] ETA: 00:00:07:42 loss: 0.477\n",
      "[11:33:06] [20/   82] ETA: 00:00:06:18 loss: 1.024\n",
      "[11:33:09] [30/   82] ETA: 00:00:05:46 loss: 0.090\n",
      "[11:33:11] [40/   82] ETA: 00:00:05:29 loss: 0.551\n",
      "[11:33:14] [50/   82] ETA: 00:00:05:17 loss: 0.103\n",
      "[11:33:16] [60/   82] ETA: 00:00:05:08 loss: 0.328\n",
      "[11:33:18] [70/   82] ETA: 00:00:05:01 loss: 0.760\n",
      "[11:33:21] [80/   82] ETA: 00:00:04:55 loss: 1.862\n",
      "[11:33:21] [82/   82] ETA: 00:00:04:54 loss: 0.008\n",
      "Classification Accuracy of the model on the test images(mAcc): 0.9939 %\n",
      "Finished Testing\n",
      "Finish Epoch 9\\15 with acc: 0.998\n",
      "Epoch 10\\15\n",
      "[11:33:29] [10/  328] ETA: 10:16:20:57 loss: 0.010\n",
      "[11:33:34] [20/  328] ETA: 05:08:17:47 loss: 0.004\n",
      "[11:33:40] [30/  328] ETA: 03:13:36:36 loss: 0.010\n",
      "[11:33:46] [40/  328] ETA: 02:16:15:57 loss: 0.044\n",
      "[11:33:51] [50/  328] ETA: 02:03:27:29 loss: 0.015\n",
      "[11:33:57] [60/  328] ETA: 01:18:55:18 loss: 0.016\n",
      "[11:34:02] [70/  328] ETA: 01:12:49:18 loss: 0.017\n",
      "[11:34:08] [80/  328] ETA: 01:08:14:47 loss: 0.022\n",
      "[11:34:14] [90/  328] ETA: 01:04:41:15 loss: 0.008\n",
      "[11:34:19] [100/  328] ETA: 01:01:50:25 loss: 0.006\n",
      "[11:34:25] [110/  328] ETA: 00:23:30:36 loss: 0.013\n",
      "[11:34:31] [120/  328] ETA: 00:21:34:13 loss: 0.008\n",
      "[11:34:36] [130/  328] ETA: 00:19:55:42 loss: 0.011\n",
      "[11:34:42] [140/  328] ETA: 00:18:31:12 loss: 0.016\n",
      "[11:34:48] [150/  328] ETA: 00:17:17:55 loss: 0.006\n",
      "[11:34:53] [160/  328] ETA: 00:16:13:47 loss: 0.021\n",
      "[11:34:59] [170/  328] ETA: 00:15:17:12 loss: 0.022\n",
      "[11:35:04] [180/  328] ETA: 00:14:26:53 loss: 0.015\n",
      "[11:35:10] [190/  328] ETA: 00:13:41:52 loss: 0.027\n",
      "[11:35:16] [200/  328] ETA: 00:13:01:20 loss: 0.007\n",
      "[11:35:21] [210/  328] ETA: 00:12:24:39 loss: 0.014\n",
      "[11:35:27] [220/  328] ETA: 00:11:51:17 loss: 0.010\n",
      "[11:35:33] [230/  328] ETA: 00:11:20:50 loss: 0.005\n",
      "[11:35:38] [240/  328] ETA: 00:10:52:53 loss: 0.007\n",
      "[11:35:44] [250/  328] ETA: 00:10:27:11 loss: 0.007\n",
      "[11:35:49] [260/  328] ETA: 00:10:03:26 loss: 0.007\n",
      "[11:35:55] [270/  328] ETA: 00:09:41:27 loss: 0.010\n",
      "[11:36:01] [280/  328] ETA: 00:09:21:03 loss: 0.013\n",
      "[11:36:06] [290/  328] ETA: 00:09:02:02 loss: 0.014\n",
      "[11:36:12] [300/  328] ETA: 00:08:44:17 loss: 0.010\n",
      "[11:36:17] [310/  328] ETA: 00:08:27:39 loss: 0.013\n",
      "[11:36:23] [320/  328] ETA: 00:08:12:03 loss: 0.004\n",
      "[11:36:27] [328/  328] ETA: 00:08:00:12 loss: 0.008\n",
      "Start Testing at 11:36:27\n",
      "[11:36:31] [10/   82] ETA: 00:00:07:44 loss: 0.169\n",
      "[11:36:34] [20/   82] ETA: 00:00:06:21 loss: 1.203\n",
      "[11:36:36] [30/   82] ETA: 00:00:05:47 loss: 0.184\n",
      "[11:36:38] [40/   82] ETA: 00:00:05:29 loss: 0.462\n",
      "[11:36:41] [50/   82] ETA: 00:00:05:20 loss: 0.057\n",
      "[11:36:43] [60/   82] ETA: 00:00:05:12 loss: 0.262\n",
      "[11:36:46] [70/   82] ETA: 00:00:05:04 loss: 0.950\n",
      "[11:36:48] [80/   82] ETA: 00:00:04:57 loss: 2.106\n",
      "[11:36:49] [82/   82] ETA: 00:00:04:56 loss: 0.013\n",
      "Classification Accuracy of the model on the test images(mAcc): 0.9931 %\n",
      "Finished Testing\n",
      "Finish Epoch 10\\15 with acc: 0.999\n",
      "Epoch 11\\15\n",
      "[11:36:56] [10/  328] ETA: 11:20:33:27 loss: 0.006\n",
      "[11:37:01] [20/  328] ETA: 05:22:22:11 loss: 0.006\n",
      "[11:37:07] [30/  328] ETA: 03:22:58:38 loss: 0.009\n",
      "[11:37:12] [40/  328] ETA: 02:23:16:40 loss: 0.006\n",
      "[11:37:18] [50/  328] ETA: 02:09:03:33 loss: 0.004\n",
      "[11:37:24] [60/  328] ETA: 01:23:34:42 loss: 0.004\n",
      "[11:37:29] [70/  328] ETA: 01:16:48:22 loss: 0.006\n",
      "[11:37:35] [80/  328] ETA: 01:11:43:38 loss: 0.005\n",
      "[11:37:41] [90/  328] ETA: 01:07:46:33 loss: 0.008\n",
      "[11:37:46] [100/  328] ETA: 01:04:36:52 loss: 0.003\n",
      "[11:37:52] [110/  328] ETA: 01:02:01:41 loss: 0.006\n",
      "[11:37:58] [120/  328] ETA: 00:23:52:18 loss: 0.008\n",
      "[11:38:03] [130/  328] ETA: 00:22:02:50 loss: 0.004\n",
      "[11:38:09] [140/  328] ETA: 00:20:29:02 loss: 0.004\n",
      "[11:38:15] [150/  328] ETA: 00:19:07:41 loss: 0.003\n",
      "[11:38:20] [160/  328] ETA: 00:17:56:30 loss: 0.004\n",
      "[11:38:26] [170/  328] ETA: 00:16:53:38 loss: 0.006\n",
      "[11:38:32] [180/  328] ETA: 00:15:57:47 loss: 0.009\n",
      "[11:38:37] [190/  328] ETA: 00:15:07:49 loss: 0.007\n",
      "[11:38:43] [200/  328] ETA: 00:14:22:48 loss: 0.007\n",
      "[11:38:48] [210/  328] ETA: 00:13:42:04 loss: 0.015\n",
      "[11:38:54] [220/  328] ETA: 00:13:05:04 loss: 0.004\n",
      "[11:39:00] [230/  328] ETA: 00:12:31:14 loss: 0.013\n",
      "[11:39:05] [240/  328] ETA: 00:12:00:13 loss: 0.007\n",
      "[11:39:11] [250/  328] ETA: 00:11:31:43 loss: 0.011\n",
      "[11:39:17] [260/  328] ETA: 00:11:05:21 loss: 0.004\n",
      "[11:39:22] [270/  328] ETA: 00:10:40:57 loss: 0.009\n",
      "[11:39:28] [280/  328] ETA: 00:10:18:16 loss: 0.003\n",
      "[11:39:33] [290/  328] ETA: 00:09:57:09 loss: 0.003\n",
      "[11:39:39] [300/  328] ETA: 00:09:37:27 loss: 0.003\n",
      "[11:39:45] [310/  328] ETA: 00:09:19:01 loss: 0.003\n",
      "[11:39:50] [320/  328] ETA: 00:09:01:42 loss: 0.002\n",
      "[11:39:54] [328/  328] ETA: 00:08:48:32 loss: 0.006\n",
      "Start Testing at 11:39:55\n",
      "[11:39:58] [10/   82] ETA: 00:00:07:27 loss: 0.172\n",
      "[11:40:01] [20/   82] ETA: 00:00:06:09 loss: 1.257\n",
      "[11:40:03] [30/   82] ETA: 00:00:05:39 loss: 0.060\n",
      "[11:40:05] [40/   82] ETA: 00:00:05:26 loss: 0.579\n",
      "[11:40:08] [50/   82] ETA: 00:00:05:15 loss: 0.034\n",
      "[11:40:10] [60/   82] ETA: 00:00:05:10 loss: 0.482\n",
      "[11:40:13] [70/   82] ETA: 00:00:05:03 loss: 0.607\n",
      "[11:40:15] [80/   82] ETA: 00:00:04:57 loss: 2.314\n",
      "[11:40:16] [82/   82] ETA: 00:00:04:55 loss: 0.010\n",
      "Classification Accuracy of the model on the test images(mAcc): 0.9931 %\n",
      "Finished Testing\n",
      "Finish Epoch 11\\15 with acc: 0.999\n",
      "Epoch 12\\15\n",
      "[11:40:23] [10/  328] ETA: 13:00:48:27 loss: 0.003\n",
      "[11:40:28] [20/  328] ETA: 06:12:28:10 loss: 0.002\n",
      "[11:40:34] [30/  328] ETA: 04:08:21:20 loss: 0.006\n",
      "[11:40:40] [40/  328] ETA: 03:06:17:46 loss: 0.003\n",
      "[11:40:45] [50/  328] ETA: 02:14:39:56 loss: 0.005\n",
      "[11:40:51] [60/  328] ETA: 02:04:14:27 loss: 0.005\n",
      "[11:40:57] [70/  328] ETA: 01:20:47:37 loss: 0.002\n",
      "[11:41:02] [80/  328] ETA: 01:15:12:27 loss: 0.011\n",
      "[11:41:08] [90/  328] ETA: 01:10:51:46 loss: 0.007\n",
      "[11:41:13] [100/  328] ETA: 01:07:23:13 loss: 0.004\n",
      "[11:41:19] [110/  328] ETA: 01:04:32:32 loss: 0.003\n",
      "[11:41:25] [120/  328] ETA: 01:02:10:22 loss: 0.002\n",
      "[11:41:30] [130/  328] ETA: 01:00:10:00 loss: 0.003\n",
      "[11:41:36] [140/  328] ETA: 00:22:26:47 loss: 0.003\n",
      "[11:41:42] [150/  328] ETA: 00:20:57:22 loss: 0.005\n",
      "[11:41:47] [160/  328] ETA: 00:19:39:07 loss: 0.004\n",
      "[11:41:53] [170/  328] ETA: 00:18:30:05 loss: 0.005\n",
      "[11:41:59] [180/  328] ETA: 00:17:28:40 loss: 0.003\n",
      "[11:42:04] [190/  328] ETA: 00:16:33:42 loss: 0.008\n",
      "[11:42:10] [200/  328] ETA: 00:15:44:13 loss: 0.002\n",
      "[11:42:16] [210/  328] ETA: 00:14:59:29 loss: 0.003\n",
      "[11:42:21] [220/  328] ETA: 00:14:18:47 loss: 0.002\n",
      "[11:42:27] [230/  328] ETA: 00:13:41:35 loss: 0.004\n",
      "[11:42:32] [240/  328] ETA: 00:13:07:29 loss: 0.003\n",
      "[11:42:38] [250/  328] ETA: 00:12:36:06 loss: 0.014\n",
      "[11:42:43] [260/  328] ETA: 00:12:07:08 loss: 0.002\n",
      "[11:42:49] [270/  328] ETA: 00:11:40:21 loss: 0.004\n",
      "[11:42:55] [280/  328] ETA: 00:11:15:26 loss: 0.004\n",
      "[11:43:00] [290/  328] ETA: 00:10:52:13 loss: 0.003\n",
      "[11:43:06] [300/  328] ETA: 00:10:30:33 loss: 0.005\n",
      "[11:43:12] [310/  328] ETA: 00:10:10:17 loss: 0.002\n",
      "[11:43:17] [320/  328] ETA: 00:09:51:16 loss: 0.003\n",
      "[11:43:21] [328/  328] ETA: 00:09:36:50 loss: 0.012\n",
      "Start Testing at 11:43:22\n",
      "[11:43:25] [10/   82] ETA: 00:00:07:42 loss: 0.473\n",
      "[11:43:28] [20/   82] ETA: 00:00:06:12 loss: 1.504\n",
      "[11:43:30] [30/   82] ETA: 00:00:05:46 loss: 0.035\n",
      "[11:43:33] [40/   82] ETA: 00:00:05:30 loss: 0.376\n",
      "[11:43:35] [50/   82] ETA: 00:00:05:18 loss: 0.071\n",
      "[11:43:37] [60/   82] ETA: 00:00:05:09 loss: 0.292\n",
      "[11:43:40] [70/   82] ETA: 00:00:05:03 loss: 0.494\n",
      "[11:43:42] [80/   82] ETA: 00:00:04:57 loss: 2.189\n",
      "[11:43:43] [82/   82] ETA: 00:00:04:55 loss: 0.004\n",
      "Classification Accuracy of the model on the test images(mAcc): 0.9931 %\n",
      "Finished Testing\n",
      "Finish Epoch 12\\15 with acc: 0.999\n",
      "Epoch 13\\15\n",
      "[11:43:50] [10/  328] ETA: 14:05:03:03 loss: 0.005\n",
      "[11:43:55] [20/  328] ETA: 07:02:33:32 loss: 0.004\n",
      "[11:44:01] [30/  328] ETA: 04:17:43:43 loss: 0.002\n",
      "[11:44:07] [40/  328] ETA: 03:13:18:45 loss: 0.001\n",
      "[11:44:12] [50/  328] ETA: 02:20:15:41 loss: 0.003\n",
      "[11:44:18] [60/  328] ETA: 02:08:53:36 loss: 0.003\n",
      "[11:44:23] [70/  328] ETA: 02:00:46:27 loss: 0.001\n",
      "[11:44:29] [80/  328] ETA: 01:18:41:02 loss: 0.003\n",
      "[11:44:35] [90/  328] ETA: 01:13:56:47 loss: 0.002\n",
      "[11:44:40] [100/  328] ETA: 01:10:09:22 loss: 0.005\n",
      "[11:44:46] [110/  328] ETA: 01:07:03:15 loss: 0.002\n",
      "[11:44:52] [120/  328] ETA: 01:04:28:14 loss: 0.004\n",
      "[11:44:57] [130/  328] ETA: 01:02:16:59 loss: 0.008\n",
      "[11:45:03] [140/  328] ETA: 01:00:24:28 loss: 0.002\n",
      "[11:45:08] [150/  328] ETA: 00:22:46:59 loss: 0.004\n",
      "[11:45:14] [160/  328] ETA: 00:21:21:37 loss: 0.001\n",
      "[11:45:20] [170/  328] ETA: 00:20:06:19 loss: 0.001\n",
      "[11:45:25] [180/  328] ETA: 00:18:59:24 loss: 0.003\n",
      "[11:45:31] [190/  328] ETA: 00:17:59:29 loss: 0.004\n",
      "[11:45:37] [200/  328] ETA: 00:17:05:32 loss: 0.004\n",
      "[11:45:42] [210/  328] ETA: 00:16:16:44 loss: 0.004\n",
      "[11:45:48] [220/  328] ETA: 00:15:32:20 loss: 0.003\n",
      "[11:45:53] [230/  328] ETA: 00:14:51:48 loss: 0.003\n",
      "[11:45:59] [240/  328] ETA: 00:14:14:42 loss: 0.001\n",
      "[11:46:05] [250/  328] ETA: 00:13:40:29 loss: 0.001\n",
      "[11:46:10] [260/  328] ETA: 00:13:08:55 loss: 0.007\n",
      "[11:46:16] [270/  328] ETA: 00:12:39:41 loss: 0.003\n",
      "[11:46:21] [280/  328] ETA: 00:12:12:31 loss: 0.002\n",
      "[11:46:27] [290/  328] ETA: 00:11:47:16 loss: 0.002\n",
      "[11:46:33] [300/  328] ETA: 00:11:23:40 loss: 0.003\n",
      "[11:46:38] [310/  328] ETA: 00:11:01:34 loss: 0.004\n",
      "[11:46:44] [320/  328] ETA: 00:10:40:50 loss: 0.002\n",
      "[11:46:48] [328/  328] ETA: 00:10:25:06 loss: 0.002\n",
      "Start Testing at 11:46:48\n",
      "[11:46:52] [10/   82] ETA: 00:00:08:00 loss: 0.438\n",
      "[11:46:55] [20/   82] ETA: 00:00:06:24 loss: 1.211\n",
      "[11:46:57] [30/   82] ETA: 00:00:05:57 loss: 0.114\n",
      "[11:47:00] [40/   82] ETA: 00:00:05:37 loss: 0.352\n",
      "[11:47:02] [50/   82] ETA: 00:00:05:23 loss: 0.026\n",
      "[11:47:04] [60/   82] ETA: 00:00:05:13 loss: 0.298\n",
      "[11:47:07] [70/   82] ETA: 00:00:05:07 loss: 0.520\n",
      "[11:47:09] [80/   82] ETA: 00:00:05:00 loss: 2.175\n",
      "[11:47:10] [82/   82] ETA: 00:00:04:59 loss: 0.006\n",
      "Classification Accuracy of the model on the test images(mAcc): 0.9939 %\n",
      "Finished Testing\n",
      "Finish Epoch 13\\15 with acc: 0.999\n",
      "Epoch 14\\15\n",
      "[11:47:17] [10/  328] ETA: 15:09:16:42 loss: 0.001\n",
      "[11:47:22] [20/  328] ETA: 07:16:38:48 loss: 0.002\n",
      "[11:47:28] [30/  328] ETA: 05:03:06:24 loss: 0.001\n",
      "[11:47:34] [40/  328] ETA: 03:20:19:55 loss: 0.002\n",
      "[11:47:39] [50/  328] ETA: 03:01:51:53 loss: 0.003\n",
      "[11:47:45] [60/  328] ETA: 02:13:33:17 loss: 0.002\n",
      "[11:47:51] [70/  328] ETA: 02:04:45:40 loss: 0.002\n",
      "[11:47:56] [80/  328] ETA: 01:22:09:56 loss: 0.002\n",
      "[11:48:02] [90/  328] ETA: 01:17:02:14 loss: 0.007\n",
      "[11:48:08] [100/  328] ETA: 01:12:55:57 loss: 0.002\n",
      "[11:48:13] [110/  328] ETA: 01:09:34:26 loss: 0.001\n",
      "[11:48:19] [120/  328] ETA: 01:06:46:28 loss: 0.003\n",
      "[11:48:25] [130/  328] ETA: 01:04:24:20 loss: 0.002\n",
      "[11:48:30] [140/  328] ETA: 01:02:22:33 loss: 0.002\n",
      "[11:48:36] [150/  328] ETA: 01:00:36:56 loss: 0.002\n",
      "[11:48:41] [160/  328] ETA: 00:23:04:29 loss: 0.003\n",
      "[11:48:47] [170/  328] ETA: 00:21:42:54 loss: 0.003\n",
      "[11:48:53] [180/  328] ETA: 00:20:30:24 loss: 0.004\n",
      "[11:48:58] [190/  328] ETA: 00:19:25:30 loss: 0.002\n",
      "[11:49:04] [200/  328] ETA: 00:18:27:07 loss: 0.002\n",
      "[11:49:10] [210/  328] ETA: 00:17:34:16 loss: 0.001\n",
      "[11:49:15] [220/  328] ETA: 00:16:46:11 loss: 0.001\n",
      "[11:49:21] [230/  328] ETA: 00:16:02:17 loss: 0.001\n",
      "[11:49:26] [240/  328] ETA: 00:15:22:03 loss: 0.003\n",
      "[11:49:32] [250/  328] ETA: 00:14:45:03 loss: 0.004\n",
      "[11:49:38] [260/  328] ETA: 00:14:10:53 loss: 0.003\n",
      "[11:49:43] [270/  328] ETA: 00:13:39:14 loss: 0.004\n",
      "[11:49:49] [280/  328] ETA: 00:13:09:50 loss: 0.001\n",
      "[11:49:55] [290/  328] ETA: 00:12:42:26 loss: 0.001\n",
      "[11:50:00] [300/  328] ETA: 00:12:16:52 loss: 0.003\n",
      "[11:50:06] [310/  328] ETA: 00:11:52:58 loss: 0.003\n",
      "[11:50:11] [320/  328] ETA: 00:11:30:31 loss: 0.003\n",
      "[11:50:16] [328/  328] ETA: 00:11:13:29 loss: 0.001\n",
      "Start Testing at 11:50:16\n",
      "[11:50:20] [10/   82] ETA: 00:00:07:49 loss: 0.600\n",
      "[11:50:22] [20/   82] ETA: 00:00:06:26 loss: 1.318\n",
      "[11:50:25] [30/   82] ETA: 00:00:05:56 loss: 0.045\n",
      "[11:50:27] [40/   82] ETA: 00:00:05:37 loss: 0.539\n",
      "[11:50:29] [50/   82] ETA: 00:00:05:24 loss: 0.084\n",
      "[11:50:32] [60/   82] ETA: 00:00:05:15 loss: 0.318\n",
      "[11:50:34] [70/   82] ETA: 00:00:05:07 loss: 0.556\n",
      "[11:50:37] [80/   82] ETA: 00:00:05:01 loss: 2.208\n",
      "[11:50:37] [82/   82] ETA: 00:00:05:00 loss: 0.005\n",
      "Classification Accuracy of the model on the test images(mAcc): 0.9924 %\n",
      "Finished Testing\n",
      "Finish Epoch 14\\15 with acc: 0.999\n",
      "Epoch 15\\15\n",
      "[11:50:44] [10/  328] ETA: 16:13:33:10 loss: 0.004\n",
      "[11:50:50] [20/  328] ETA: 08:06:45:03 loss: 0.001\n",
      "[11:50:55] [30/  328] ETA: 05:12:28:57 loss: 0.001\n",
      "[11:51:01] [40/  328] ETA: 04:03:20:53 loss: 0.003\n",
      "[11:51:06] [50/  328] ETA: 03:07:28:08 loss: 0.001\n",
      "[11:51:12] [60/  328] ETA: 02:18:13:00 loss: 0.002\n",
      "[11:51:18] [70/  328] ETA: 02:08:44:52 loss: 0.002\n",
      "[11:51:23] [80/  328] ETA: 02:01:38:44 loss: 0.002\n",
      "[11:51:29] [90/  328] ETA: 01:20:07:19 loss: 0.001\n",
      "[11:51:35] [100/  328] ETA: 01:15:42:08 loss: 0.002\n",
      "[11:51:40] [110/  328] ETA: 01:12:05:14 loss: 0.001\n",
      "[11:51:46] [120/  328] ETA: 01:09:04:23 loss: 0.003\n",
      "[11:51:51] [130/  328] ETA: 01:06:31:22 loss: 0.001\n",
      "[11:51:57] [140/  328] ETA: 01:04:20:12 loss: 0.001\n",
      "[11:52:03] [150/  328] ETA: 01:02:26:36 loss: 0.003\n",
      "[11:52:09] [160/  328] ETA: 01:00:47:11 loss: 0.002\n",
      "[11:52:14] [170/  328] ETA: 00:23:19:23 loss: 0.004\n",
      "[11:52:20] [180/  328] ETA: 00:22:01:19 loss: 0.002\n",
      "[11:52:25] [190/  328] ETA: 00:20:51:27 loss: 0.001\n",
      "[11:52:31] [200/  328] ETA: 00:19:48:35 loss: 0.001\n",
      "[11:52:37] [210/  328] ETA: 00:18:51:42 loss: 0.001\n",
      "[11:52:42] [220/  328] ETA: 00:17:59:58 loss: 0.002\n",
      "[11:52:48] [230/  328] ETA: 00:17:12:43 loss: 0.001\n",
      "[11:52:54] [240/  328] ETA: 00:16:29:24 loss: 0.005\n",
      "[11:52:59] [250/  328] ETA: 00:15:49:33 loss: 0.002\n",
      "[11:53:05] [260/  328] ETA: 00:15:12:45 loss: 0.001\n",
      "[11:53:10] [270/  328] ETA: 00:14:38:40 loss: 0.002\n",
      "[11:53:16] [280/  328] ETA: 00:14:07:02 loss: 0.003\n",
      "[11:53:22] [290/  328] ETA: 00:13:37:34 loss: 0.002\n",
      "[11:53:27] [300/  328] ETA: 00:13:10:03 loss: 0.001\n",
      "[11:53:33] [310/  328] ETA: 00:12:44:18 loss: 0.004\n",
      "[11:53:39] [320/  328] ETA: 00:12:20:08 loss: 0.002\n",
      "[11:53:43] [328/  328] ETA: 00:12:01:50 loss: 0.001\n",
      "Start Testing at 11:53:43\n",
      "[11:53:47] [10/   82] ETA: 00:00:07:37 loss: 0.470\n",
      "[11:53:49] [20/   82] ETA: 00:00:06:12 loss: 1.497\n",
      "[11:53:52] [30/   82] ETA: 00:00:05:46 loss: 0.037\n",
      "[11:53:54] [40/   82] ETA: 00:00:05:27 loss: 0.568\n",
      "[11:53:56] [50/   82] ETA: 00:00:05:16 loss: 0.068\n",
      "[11:53:59] [60/   82] ETA: 00:00:05:08 loss: 0.509\n",
      "[11:54:01] [70/   82] ETA: 00:00:05:02 loss: 1.091\n",
      "[11:54:04] [80/   82] ETA: 00:00:04:56 loss: 2.023\n",
      "[11:54:04] [82/   82] ETA: 00:00:04:55 loss: 0.007\n",
      "Classification Accuracy of the model on the test images(mAcc): 0.9916 %\n",
      "Finished Testing\n",
      "Finish Epoch 15\\15 with acc: 0.999\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch ▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_train_loss █▃▂▁▂▁▁▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   epoch_val_loss █▄▄▂▃▁▂█▁▁▂▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        train_acc ▁▄▇▇▇██▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss ▄▃█▄▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_acc ▄▅▄▇▇▇▇▁█▇▇▇█▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            epoch 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: epoch_train_loss 0.00194\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   epoch_val_loss 0.76414\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        train_acc 0.99867\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_acc 0.99162\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33meffortless-sweep-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/1844986810/WSDDN/runs/z0s1x4ya\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/1844986810/WSDDN\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240618_110146-z0s1x4ya/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "wandb.login(key='025737bf0e2deb6900256f426ca16b1fff57f95b')\n",
    "\n",
    "# os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "\n",
    "def build_dataset(batch_size, train_data_path, num_workers):\n",
    "    Transform = transforms.Compose([\n",
    "        transforms.Resize([480, 480]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    # 加载整个数据集\n",
    "    full_trainData = myDataSet(train_data_path, Transform)\n",
    "\n",
    "    # 计算拆分点\n",
    "    train_size = int(0.8 * len(full_trainData))\n",
    "    validation_size = len(full_trainData) - train_size\n",
    "\n",
    "    # 拆分数据集\n",
    "    trainData, valData = torch.utils.data.random_split(full_trainData, [train_size, validation_size])\n",
    "\n",
    "    # 创建训练和验证数据加载器\n",
    "    trainLoader = torch.utils.data.DataLoader(dataset=trainData,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=num_workers)\n",
    "\n",
    "    valLoader = torch.utils.data.DataLoader(dataset=valData,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=num_workers)\n",
    "    return trainLoader, valLoader\n",
    "\n",
    "def build_model(preCNN_name, cat_num):#, model_path, result_pkl\n",
    "    model = WSDDN(preCNN_name, cat_num)\n",
    "#     preCNN_path = ''\n",
    "#     if preCNN_name == 'VGG11':\n",
    "#         preCNN_path = r'/kaggle/input/wsddn-od/wsddn-data/model_para/vgg11_bn-6002323d.pth'\n",
    "#     elif preCNN_name == 'VGG13':\n",
    "#         preCNN_path = r'/kaggle/input/wsddn-od/wsddn-data/model_para/vgg13_bn-abd245e5.pth'\n",
    "#     elif preCNN_name == 'VGG16':\n",
    "#         preCNN_path = r'/kaggle/input/wsddn-od/wsddn-data/model_para/vgg16_bn-6c64b313.pth'\n",
    "#     else :\n",
    "#         preCNN_path = r'/kaggle/input/wsddn-od/wsddn-data/model_para/vgg19_bn-c79401a0.pth'\n",
    "#     pretrained_dict = torch.load(preCNN_path)\n",
    "#     modified_dict = model.state_dict()\n",
    "#     pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in modified_dict}\n",
    "#     modified_dict.update(pretrained_dict)\n",
    "#     model.load_state_dict(modified_dict)\n",
    "    model.load_state_dict(torch.load('/kaggle/input/wsddn-od/kind-sweep-1.pth'))\n",
    "    model.cuda()\n",
    "    return model\n",
    "\n",
    "def train(args=None):\n",
    "    with wandb.init(config=args, project=\"WSDDN\"):\n",
    "        args = wandb.config\n",
    "        trainLoader, valLoader = build_dataset(args.batch_size, args.train_data_path, args.num_workers)\n",
    "        wsddn = build_model(args.preCNN_name, args.cat_num)  #, args.model_path, args.result_name\n",
    "\n",
    "        # for m in wsddn.children():\n",
    "        #     m.register_forward_hook(hook=WSDDN.hook_forward_fn)\n",
    "        #     m.register_full_backward_hook(hook=WSDDN.hook_backward_fn)\n",
    "\n",
    "        optimizer = optim.Adam(wsddn.parameters(), lr=args.learn_rate, weight_decay=args.weight_decay)\n",
    "#         optimizer = optim.SGD(wsddn.parameters(), lr=args.learn_rate, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "        # cross_entropy_Loss = nn.BCELoss(weight=None, reduction='mean')\n",
    "        ###开始训练\n",
    "        start_time = time.time()\n",
    "        print('Start Training at {}'.format(datetime.now().strftime('%H:%M:%S')))\n",
    "        wsddn.train()  # Set the model to training mode\n",
    "        max_acc = 0.0\n",
    "        for epoch in range(args.epochs):\n",
    "            correct = 0\n",
    "            epoch_loss = 0.0\n",
    "            running_loss = 0.0\n",
    "            print(f'Epoch {epoch + 1}\\{args.epochs}')\n",
    "            for i, (images, bbox, labels) in enumerate(trainLoader):\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "                bbox = bbox.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                y, _, _ = wsddn(images, bbox)\n",
    "                y = torch.clamp(y, min=0.0, max=1.0)\n",
    "                loss_ = torch.mul(labels, torch.log(y + 1e-8)) + torch.mul((1 - labels), torch.log((1 - y) + 1e-8))\n",
    "                loss = -torch.sum(loss_)\n",
    "#                 l2_loss = sum(torch.norm(p).pow(2) for p in wsddn.parameters()) * args.reg  # 添加正则项\n",
    "#                 loss += l2_loss\n",
    "                # register_hook(save_grad('y'))\n",
    "                loss.backward()  # Compute the gradient\n",
    "                optimizer.step()  # Update the weights\n",
    "                running_loss += loss.item()\n",
    "                correct = cal_correct(y, labels, correct)\n",
    "                wandb.log({\"train_loss\": loss.item(), \"epoch\": epoch + 1})\n",
    "                count = 10\n",
    "                if i % count == count - 1 or i == len(trainLoader) - 1:\n",
    "                    if i == len(trainLoader) - 1:\n",
    "                        count = i % count\n",
    "                    if count == 0:\n",
    "                        continue\n",
    "                    # 计算当前进度\n",
    "                    progress = (args.epochs * len(trainLoader)) / (i + 1)\n",
    "                    # 计算当前用去的时间\n",
    "                    time_elapsed = time.time() - start_time\n",
    "                    # 估计整个的完成时间\n",
    "                    time_finishing = time_elapsed * progress\n",
    "                    time_remaining = time_finishing - time_elapsed\n",
    "                    # 将剩余时间转换为小时和分钟\n",
    "                    days, remainder = divmod(time_remaining, 86400)\n",
    "                    hours, remainder = divmod(remainder, 3600)\n",
    "                    minutes, seconds = divmod(remainder, 60)\n",
    "                    # 打印ETA\n",
    "                    print('[%s] [%d/%5d] ETA: %02d:%02d:%02d:%02d loss: %.3f' % (\n",
    "                        datetime.now().strftime('%H:%M:%S'),\n",
    "                        i + 1,\n",
    "                        len(trainLoader),\n",
    "                        days,\n",
    "                        hours,\n",
    "                        minutes,\n",
    "                        int(seconds),\n",
    "                        running_loss / count\n",
    "                    ))\n",
    "                    epoch_loss += running_loss\n",
    "                    running_loss = 0.0\n",
    "            epoch_loss = epoch_loss / float(len(trainLoader))\n",
    "            acc = float(correct) / float(len(trainLoader) * args.batch_size)\n",
    "            # 检查是否是目前为止最高的准确率，并更新保存的模型\n",
    "            epoch_val_loss, val_acc = test(args, wsddn, valLoader)\n",
    "            if val_acc > max_acc:\n",
    "                max_acc = val_acc\n",
    "                torch.save(wsddn.state_dict(), os.path.join(args.model_path, args.result_name))\n",
    "            wandb.log({\"train_acc\": acc, \"val_acc\": val_acc, \"epoch_train_loss\": epoch_loss,\n",
    "                       \"epoch_val_loss\": epoch_val_loss})  #\n",
    "            print(f'Finish Epoch {epoch + 1}\\{args.epochs} with acc: %.3f' % acc)\n",
    "        print('Finished Training')\n",
    "        wandb.finish()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def test(args, wsddn, loader):\n",
    "    with torch.no_grad():\n",
    "        wsddn.eval()\n",
    "        correct = 0\n",
    "        val_loss = 0.0\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "        print('Start Testing at {}'.format(datetime.now().strftime('%H:%M:%S')))\n",
    "        for i, (images, bbox, labels) in enumerate(loader):\n",
    "            images = images.cuda()\n",
    "            bbox = bbox.cuda()\n",
    "            labels = labels.cuda()\n",
    "            y, _, _ = wsddn(images, bbox)\n",
    "            y = torch.clamp(y, min=0.0, max=1.0)\n",
    "            # loss_ = torch.mul(labels, y - 0.5) + 0.5\n",
    "            # loss = -torch.sum(torch.log(loss_ + 1e-8))\n",
    "            loss_ = torch.mul(labels, torch.log(y + 1e-8)) + torch.mul((1 - labels), torch.log((1 - y) + 1e-8))\n",
    "            loss = -torch.sum(loss_)\n",
    "#             l2_loss = sum(torch.norm(p).pow(2) for p in wsddn.parameters()) * args.reg  # 添加正则项\n",
    "#             loss += l2_loss\n",
    "            running_loss += loss.item()\n",
    "            # wandb.log({\"test_loss\": loss.item()})\n",
    "            # y[1,102],segma_c[1,R,102],segma_d[1,R,102]，#image= [1, c, h, w]  bbox= [1, R, 4]\n",
    "            correct = cal_correct(y, labels, correct)\n",
    "            count = 10\n",
    "            if i % count == count - 1 or i == len(loader) - 1:\n",
    "                if i == len(loader) - 1:\n",
    "                    count = i % count\n",
    "                if count == 0:\n",
    "                    continue\n",
    "                # 计算当前进度的倒数\n",
    "                progress = (args.epochs * len(loader)) / (i + 1)\n",
    "                # 计算当前用去的时间\n",
    "                time_elapsed = time.time() - start_time\n",
    "                # 估计完成时间\n",
    "                time_finishing = time_elapsed * progress\n",
    "                time_remaining = time_finishing - time_elapsed\n",
    "                # 将剩余时间转换为天、小时、分钟和秒\n",
    "                days, remainder = divmod(time_remaining, 86400)  # 86400秒 = 24小时\n",
    "                hours, remainder = divmod(remainder, 3600)  # 3600秒 = 1小时\n",
    "                minutes, seconds = divmod(remainder, 60)  # 60秒 = 1分钟\n",
    "                # 打印ETA\n",
    "                print('[%s] [%d/%5d] ETA: %02d:%02d:%02d:%02d loss: %.3f' % (\n",
    "                    datetime.now().strftime('%H:%M:%S'),\n",
    "                    i + 1,\n",
    "                    len(loader),\n",
    "                    days,\n",
    "                    hours,\n",
    "                    minutes,\n",
    "                    int(seconds),\n",
    "                    running_loss / count\n",
    "                ))\n",
    "                val_loss += running_loss\n",
    "                running_loss = 0.0\n",
    "\n",
    "        acc = float(correct) / float(len(loader) * args.batch_size)\n",
    "        print('Classification Accuracy of the model on the test images(mAcc): %.4f %%' % acc)\n",
    "        print('Finished Testing')\n",
    "        return val_loss / float(len(loader)), acc\n",
    "\n",
    "\n",
    "def cal_correct(y, labels, correct):\n",
    "    # 将预测结果更改为分类标签对应的one-hot格式\n",
    "    for i in range(y.size(0)):\n",
    "        max_indices = torch.argmax(y[i])\n",
    "        predicted = torch.zeros_like(y[i], dtype=torch.bool)\n",
    "        predicted[max_indices] = 1\n",
    "        # 把每个元素相加得到分类正确的数量\n",
    "        correct += 1 if torch.equal(predicted, labels[i]) else 0\n",
    "    return correct\n",
    "\n",
    "# torch.cuda.set_device(0)\n",
    "# args=parse_args()\n",
    "# train(args)\n",
    "\n",
    "# with open('/kaggle/working/wsddn-project/sweep-random-hyperband.yaml', 'r') as file:\n",
    "#     sweep_config = yaml.safe_load(file)\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"WSDDN\")\n",
    "sweep_configuration = {\n",
    "    \"method\": \"grid\",\n",
    "    \"metric\": {\"goal\": \"minimize\", \"name\": \"train_loss\"},\n",
    "    \"parameters\": {\n",
    "        \"model_path\":{\"value\":'/kaggle/working/'},\n",
    "        \"result_name\":{\"value\":'wsddn.pth'},\n",
    "        \"batch_size\":{\"value\":16},\n",
    "        \"learn_rate\":{\"value\":6.4e-6},\n",
    "        \"epochs\":{\"value\":15},\n",
    "        \"train_data_path\":{\"value\":'/kaggle/input/wsddn-od/ssw_train.txt'},\n",
    "        \"val_data_path\":{\"value\":'/kaggle/input/wsddn-od/ssw_val.txt'},\n",
    "        \"preCNN_name\":{\"value\":'VGG11'},\n",
    "        \"cat_num\":{\"value\":102},\n",
    "        \"momentum\":{\"value\":0},\n",
    "        \"weight_decay\":{\"value\":1e-7},\n",
    "        \"num_workers\":{\"value\":8}\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"WSDDN\")\n",
    "wandb.agent(sweep_id, function=train, project=\"WSDDN\", count=6)\n",
    "# print(\"what fuck\")\n",
    "# wandb.agent('yun6p7im', function=train, count=140, project=\"WSDDN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c74e2c67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T11:54:19.776226Z",
     "iopub.status.busy": "2024-06-18T11:54:19.775232Z",
     "iopub.status.idle": "2024-06-18T11:54:19.787311Z",
     "shell.execute_reply": "2024-06-18T11:54:19.786410Z"
    },
    "papermill": {
     "duration": 0.070984,
     "end_time": "2024-06-18T11:54:19.789265",
     "exception": false,
     "start_time": "2024-06-18T11:54:19.718281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(args=None):\n",
    "    sizes = [480, 688, 1200]\n",
    "    all_transforms = []\n",
    "    for size in sizes:\n",
    "        resize = transforms.Resize((size, size))\n",
    "        for horizontal_flip in [True, False]:\n",
    "            for vertical_flip in [True, False]:\n",
    "                horizontal_flip_transform = transforms.RandomHorizontalFlip(p=1 if horizontal_flip else 0)\n",
    "                vertical_flip_transform = transforms.RandomVerticalFlip(p=1 if vertical_flip else 0)\n",
    "                combined_transform = transforms.Compose([\n",
    "                    resize,\n",
    "                    horizontal_flip_transform,\n",
    "                    vertical_flip_transform,\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225]),\n",
    "                ])\n",
    "                all_transforms.append(combined_transform)\n",
    "    with wandb.init(config=args, project=\"WSDDN\"):\n",
    "        args = wandb.config\n",
    "        testData=torch.utils.data.ConcatDataset([])\n",
    "        for Transform in all_transforms:\n",
    "            testData.add_dataset(myDataSet(args.val_data_path, Transform))\n",
    "        loader = torch.utils.data.DataLoader(dataset=testData,\n",
    "                                             batch_size=args.batch_size,\n",
    "                                             shuffle=False)\n",
    "        wsddn = WSDDN(args.preCNN_name, args.cat_num)\n",
    "        wsddn.load_state_dict(torch.load(os.path.join(args.model_path, args.result_pkl)))\n",
    "        wsddn.cuda()\n",
    "        test(args, wsddn, loader)\n",
    "        \n",
    "# sweep_configuration = {\n",
    "#     \"method\": \"grid\",\n",
    "#     \"metric\": {\"goal\": \"minimize\", \"name\": \"train_loss\"},\n",
    "#     \"parameters\": {\n",
    "#         \"model_path\":{\"value\":'/kaggle/working/'},\n",
    "#         \"result_name\":{\"value\":'wsddn.pth'},\n",
    "#         \"batch_size\":{\"value\":16},\n",
    "#         \"learn_rate\":{\"value\": 0.0000306},\n",
    "#         \"epochs\":{\"value\":20},\n",
    "#         \"train_data_path\":{\"value\":'/kaggle/input/wsddn-od/ssw_train.txt'},\n",
    "#         \"val_data_path\":{\"value\":'/kaggle/input/wsddn-od/ssw_val.txt'},\n",
    "#         \"preCNN_name\":{\"value\":'VGG11'},\n",
    "#         \"cat_num\":{\"value\":102},\n",
    "#         \"momentum\":{\"value\":0},\n",
    "#         \"weight_decay\":{\"value\":1e-7},\n",
    "#         \"num_workers\":{\"value\":8}\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"WSDDN\")\n",
    "# wandb.agent(sweep_id, function=evaluate, project=\"WSDDN\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5044549,
     "sourceId": 8571605,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3170.739165,
   "end_time": "2024-06-18T11:54:23.411665",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-18T11:01:32.672500",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
